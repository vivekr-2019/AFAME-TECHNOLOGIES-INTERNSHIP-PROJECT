# AFAME-TECHNOLOGIES-INTERNSHIP-PROJECT

# DATA ANALYST INTERNSHIP PROJECT

## TITANIC SURVIVAL PREDICTION

- The sinking of the Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank  after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone on board,  resulting in the death of 1502 out of 2224 passengers and crew. 
- Using the Titanic dataset to build a model that predicts whether a passenger on the Titanic survived or  not. This is a classic beginner project with readily available data. The dataset typically used for this project contains information about individual passengers, such as their age, gender, ticket class, fare, cabin, and whether or not they survived

### Dataset

The Titanic dataset used in this project contains information about individual passengers, such as their age, gender, ticket class, fare, cabin, and whether or not they survived. The dataset can be obtained from [(https://drive.google.com/drive/folders/1SBScS_ixyyh4VtI3DaCbmgvCflgdyyJv?usp=sharing)].

### Dependencies

The following Python libraries were used in this project:

- Pandas
- NumPy
- Scikit-learn
- Matplotlib
- Seaborn
- XGBoost

### Development and Deployment

This project was developed and deployed using the Deepnote cloud environment.

### Project Overview

The project involves the following steps:

1. **Data Exploration**: Exploring the dataset, understanding its features, and performing initial data analysis.
2. **Data Preprocessing**: Handling missing values, encoding categorical variables, and scaling numerical features.
3. **Feature Engineering**: Creating new features or transforming existing ones to improve the model's performance.
4. **Model Selection and Training**: Trying different machine learning algorithms and training models on the preprocessed data.
5. **Model Evaluation**: Evaluating the performance of the trained models using appropriate metrics.
6. **Model Optimization**: Tuning the hyperparameters of the best-performing model to further improve its accuracy.
7. **Deployment**: Deploying the final model for making predictions on new data.

### Usage

1. Clone the repository or download the project files.
2. Install the required dependencies using `pip install xgboost`, `!pip install lightgbm==4.3.0`.
3. Run the `Intern Project 1_ Titanic Dataset.ipynb` script, which contains the code for the complete project.
4. The trained model will be saved, and predictions can be made on new data.

## HR Data Analysis

This project focuses on performing data cleansing and preprocessing on an HR dataset. The main tasks involved are:

1. **Removing unnecessary columns**: Identifying and dropping columns that are not relevant to the analysis.

2. **Renaming columns**: Providing more descriptive and meaningful names to the columns for better understanding.

3. **Eliminating redundant entries**: Removing any duplicate or redundant rows from the dataset.

4. **Sanitizing specific columns**: Cleaning and formatting columns that contain inconsistent or incorrect data.

5. **Handling missing values**: Dealing with NaN (Not a Number) or null values in the dataset using appropriate techniques like imputation or removal.

6. **Additional data cleaning steps**: Performing any other necessary data cleaning and preprocessing tasks based on the specific requirements of the dataset.


### Dataset

The HR dataset used in this project can be obtained from [https://drive.google.com/drive/folders/1SBScS_ixyyh4VtI3DaCbmgvCflgdyyJv?usp=sharing].

### Dependencies

The following Python libraries were used in this project:

- Pandas
- NumPy
- Seaborn
- Matplotlib

### Development and Deployment

This project was developed and deployed using the Deepnote cloud environment.

### Usage

1. Clone the repository or download the project files.
2. Install the required dependencies using `!pip install lightgbm==4.3.0`, ` !pip install catboost==1.2.5`.
3. Run the `Intern Project 1_ Titanic Dataset.ipynb` notebook, which contains the complete code for the project.
4. The cleaned and preprocessed dataset will be saved as a CSV file in the same directory.


